import streamlit as st
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import esm
from mamba_ssm import Mamba
from sklearn.preprocessing import StandardScaler
import time
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


# ==========================================
# 1. æ ¸å¿ƒæ¨¡å‹æ¶æ„ (ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´)
# ==========================================
class CNNBranch(nn.Module):
    def __init__(self, input_dim=480, num_classes=8): # ä¿®æ”¹ input_dim
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.Unflatten(1, (1, 256)),
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(256, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.AdaptiveMaxPool1d(1)
        )
        self.classifier = nn.Linear(128, num_classes)


    def forward(self, x):
        return self.classifier(self.net(x).flatten(1))


class TransformerBranch(nn.Module):
    def __init__(self, input_dim=480, d_model=256, nhead=8, num_classes=8): # ä¿®æ”¹ input_dim
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, dropout=0.2)
        self.transformer = nn.TransformerEncoder(layer, num_layers=4)
        self.classifier = nn.Linear(d_model, num_classes)


    def forward(self, x):
        x = self.embedding(x).unsqueeze(1)
        return self.classifier(self.transformer(x).squeeze(1))


class MambaBranch(nn.Module):
    def __init__(self, input_dim=480, num_classes=8): # ä¿®æ”¹ input_dim
        super().__init__()
        self.preprocess = nn.Linear(input_dim, 256)
        self.mamba_blocks = nn.ModuleList([Mamba(d_model=256, d_state=16, d_conv=4, expand=2) for _ in range(5)])
        self.norm = nn.LayerNorm(256)
        self.classifier = nn.Linear(256, num_classes)


    def forward(self, x):
        x = self.preprocess(x).unsqueeze(1)
        for block in self.mamba_blocks:
            x = x + block(x)
        return self.classifier(self.norm(x).squeeze(1))


class MutualLearningModel(nn.Module):
    def __init__(self, input_dim=480, num_classes=8, embed_dim=128): # ä¿®æ”¹ input_dim
        super().__init__()
        self.cnn = CNNBranch(input_dim, num_classes)
        self.trans = TransformerBranch(input_dim, num_classes=num_classes)
        self.mamba = MambaBranch(input_dim, num_classes)
        self.logits_norm = nn.LayerNorm(num_classes)
        self.feature_proj = nn.Sequential(
            nn.Linear(num_classes, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        self.blocks = nn.ModuleList([
            nn.ModuleDict({
                'attn': nn.MultiheadAttention(embed_dim, 8, dropout=0.2, batch_first=True),
                'norm1': nn.LayerNorm(embed_dim),
                'ffn': nn.Sequential(
                    nn.Linear(embed_dim, embed_dim*4),
                    nn.GELU(),
                    nn.Dropout(0.2),
                    nn.Linear(embed_dim*4, embed_dim)
                ),
                'norm2': nn.LayerNorm(embed_dim)
            })
            for _ in range(2)
        ])
        self.gate = nn.Sequential(
            nn.Linear(embed_dim*3 + num_classes*3, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, 3)
        )
        self.log_temp = nn.Parameter(torch.tensor(np.log(0.8)))
        self.refine = nn.Sequential(
            nn.Linear(num_classes, num_classes),
            nn.LayerNorm(num_classes),
            nn.GELU()
        )


    def forward(self, x):
        o1, o2, o3 = self.cnn(x), self.trans(x), self.mamba(x)
        branches = torch.stack([o1, o2, o3], dim=1)
        x_f = self.feature_proj(self.logits_norm(branches))
        for b in self.blocks:
            attn_out, _ = b['attn'](x_f, x_f, x_f)
            x_f = b['norm1'](x_f + attn_out)
            x_f = b['norm2'](x_f + b['ffn'](x_f))
        gate_input = torch.cat([x_f.flatten(1), branches.flatten(1)], dim=1)
        temp = F.softplus(self.log_temp) + 1e-4
        weights = F.softmax(self.gate(gate_input) / temp, dim=1).unsqueeze(-1)
        o_fused = (branches * weights).sum(dim=1)
        return o1, o2, o3, o_fused + self.refine(o_fused)


# ==========================================
# 2. ESM ç‰¹å¾æå–å™¨
# ==========================================
class ESMFeatureExtractor:
    def __init__(self):
        self.gpu_model = None
        self.gpu_batch_converter = None
        self.cpu_model = None
        self.cpu_batch_converter = None
        self.device = None
        self._initialize_models()


    def _initialize_models(self):
        try:
            if torch.cuda.is_available():
                print("ğŸš€ å°è¯•åŠ è½½GPUæ¨¡å‹ï¼ˆESM-2 35Mï¼‰...")
                self.gpu_model, alphabet = esm.pretrained.esm2_t6_35M_UR50D() # æ›¿æ¢ä¸º 35M æ¨¡å‹
                self.gpu_device = torch.device('cuda')
                self.gpu_model = self.gpu_model.to(self.gpu_device)
                self.gpu_batch_converter = alphabet.get_batch_converter()
                self.device = self.gpu_device
                print("âœ… GPUæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ GPUæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        try:
            print("ğŸ–¥ï¸ åŠ è½½CPUæ¨¡å‹ä½œä¸ºå¤‡ç”¨...")
            self.cpu_model, alphabet = esm.pretrained.esm2_t6_35M_UR50D() # æ›¿æ¢ä¸º 35M æ¨¡å‹
            self.cpu_device = torch.device('cpu')
            self.cpu_model = self.cpu_model.to(self.cpu_device)
            self.cpu_batch_converter = alphabet.get_batch_converter()
            if self.device is None:
                self.device = self.cpu_device
            print("âœ… CPUæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ CPUæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise


    def _extract_batch_features(self, batch_data, use_gpu=True):
        try:
            model = self.gpu_model if use_gpu and self.gpu_model else self.cpu_model
            batch_converter = self.gpu_batch_converter if use_gpu and self.gpu_model else self.cpu_batch_converter
            device = self.gpu_device if use_gpu and self.gpu_model else self.cpu_device


            _, _, batch_tokens = batch_converter(batch_data)
            batch_tokens = batch_tokens.to(device)
            with torch.no_grad():
                results = model(batch_tokens, repr_layers=[6], return_contacts=False) # ä¿®æ”¹ä¸ºç¬¬6å±‚
                token_representations = results["representations"][6] # ä¿®æ”¹ä¸ºç¬¬6å±‚
            seq_lengths = (batch_tokens != model.alphabet.padding_idx).sum(1)
            batch_features = [token_representations[i, :seq_lengths[i]].mean(0).cpu().numpy() for i in range(token_representations.size(0))]


            del batch_tokens, results
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return batch_features
        except RuntimeError as e:
            if "CUDA out of memory" in str(e) and use_gpu:
                return self._extract_batch_features(batch_data, use_gpu=False)
            raise


    def extract_features(self, sequences, cache_path=None, batch_size=1):
        if cache_path and os.path.exists(cache_path):
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½ç‰¹å¾: {cache_path}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)


        features = []
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i+batch_size]
            batch_data = [(str(idx), seq) for idx, seq in enumerate(batch)]
            features.extend(self._extract_batch_features(batch_data))


            if (i // batch_size) % 10 == 0:
                print(f"ğŸ“Š è¿›åº¦: {min(i+batch_size, len(sequences))}/{len(sequences)}")


        features_array = np.array(features)
        if cache_path:
            with open(cache_path, 'wb') as f:
                pickle.dump(features_array, f)
        return features_array


# ==========================================
# 3. CSVå¤„ç†ä¸“ç”¨å‡½æ•°
# ==========================================
def validate_sequence(seq):
    """éªŒè¯è›‹ç™½è´¨åºåˆ—"""
    seq = seq.strip().upper()
    valid_aa = set("ACDEFGHIKLMNPQRSTVWYX")
    invalid_chars = [c for c in seq if c not in valid_aa]
    if invalid_chars:
        return False, f"æ— æ•ˆå­—ç¬¦: {', '.join(set(invalid_chars))}"
    if len(seq) < 10:
        return False, "åºåˆ—å¤ªçŸ­ (è‡³å°‘éœ€è¦10ä¸ªæ°¨åŸºé…¸)"
    if len(seq) > 10000:
        return False, "åºåˆ—å¤ªé•¿ (æœ€å¤š10000ä¸ªæ°¨åŸºé…¸)"
    return True, ""


def validate_csv_sequences(sequences, seq_names):
    """éªŒè¯CSVä¸­çš„åºåˆ—ï¼Œè¿”å›æœ‰æ•ˆåºåˆ—ç´¢å¼•å’Œé”™è¯¯ä¿¡æ¯"""
    valid_indices = []
    errors = []
    for i, seq in enumerate(sequences):
        is_valid, message = validate_sequence(seq)
        if is_valid:
            valid_indices.append(i)
        else:
            errors.append((seq_names[i], message))
    return valid_indices, errors


def parse_csv_sequences(uploaded_file):
    """
    è§£æä¸Šä¼ çš„CSVæ–‡ä»¶ï¼Œæ™ºèƒ½è¯†åˆ«åºåˆ—åˆ—å’Œåç§°åˆ—
    è¿”å›: (åºåˆ—åç§°åˆ—è¡¨, åºåˆ—åˆ—è¡¨, åŸå§‹DataFrame, åºåˆ—åˆ—å, åç§°åˆ—å)
    """
    try:
        df = pd.read_csv(uploaded_file)
        st.success(f"âœ… æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œ {len(df.columns)} åˆ—")


        # æŸ¥æ‰¾åºåˆ—åˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        seq_col = None
        name_col = None
        possible_seq_cols = ['sequence', 'seq', 'protein_sequence', 'aa_sequence', 'peptide', 'protein']
        possible_name_cols = ['name', 'id', 'protein_id', 'identifier', 'accession', 'entry']


        # æŸ¥æ‰¾åºåˆ—åˆ—
        for col in df.columns:
            if col.lower() in possible_seq_cols:
                seq_col = col
                break


        # å¦‚æœæœªæ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«"seq"çš„åˆ—
        if seq_col is None:
            for col in df.columns:
                if 'seq' in col.lower() or 'sequence' in col.lower():
                    seq_col = col
                    break


        # æŸ¥æ‰¾åç§°åˆ—
        for col in df.columns:
            if col.lower() in possible_name_cols:
                name_col = col
                break


        # å¦‚æœä»æœªæ‰¾åˆ°åºåˆ—åˆ—ï¼ŒæŠ¥é”™
        if seq_col is None:
            st.error("âŒ æœªæ£€æµ‹åˆ°åºåˆ—åˆ—ã€‚è¯·ç¡®ä¿CSVåŒ…å«ä»¥ä¸‹åˆ—åä¹‹ä¸€ï¼š'Sequence', 'Seq', 'Protein_Sequence'ç­‰")
            st.info("ğŸ’¡ æç¤ºï¼šåˆ—åä¸åŒºåˆ†å¤§å°å†™ï¼Œä¸”éœ€åŒ…å«è›‹ç™½è´¨æ°¨åŸºé…¸åºåˆ—")
            return None, None, None, None, None


        # æå–åºåˆ—ï¼ˆæ¸…ç†ç©ºæ ¼å’ŒNaNï¼‰
        sequences = []
        for idx, seq in enumerate(df[seq_col]):
            if pd.isna(seq) or str(seq).strip() == "":
                st.warning(f"âš ï¸ ç¬¬ {idx+1} è¡Œåºåˆ—ä¸ºç©ºï¼Œå°†è·³è¿‡")
                sequences.append(None)
            else:
                sequences.append(str(seq).strip().upper())


        # ç”Ÿæˆåç§°åˆ—è¡¨
        if name_col is not None:
            seq_names = []
            for idx, name in enumerate(df[name_col]):
                if pd.isna(name) or str(name).strip() == "":
                    seq_names.append(f"Seq_{idx+1}")
                else:
                    seq_names.append(str(name).strip())
        else:
            seq_names = [f"Seq_{i+1}" for i in range(len(sequences))]


        # è¿‡æ»¤ç©ºåºåˆ—
        valid_indices = [i for i, seq in enumerate(sequences) if seq is not None and len(seq.strip()) > 0]
        filtered_names = [seq_names[i] for i in valid_indices]
        filtered_seqs = [sequences[i] for i in valid_indices]


        name_display = "è‡ªåŠ¨ç¼–å·" if name_col is None else f'"{name_col}"'
        st.info(f"ğŸ” æ£€æµ‹åˆ°åºåˆ—åˆ—: '{seq_col}' | åç§°åˆ—: {name_display}")
        st.info(f"âœ… æœ‰æ•ˆåºåˆ—æ•°é‡: {len(filtered_seqs)} / {len(sequences)}")


        return filtered_names, filtered_seqs, df, seq_col, name_col


    except Exception as e:
        st.error(f"âŒ è§£æCSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        st.info("ğŸ’¡ è¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„CSVæ ¼å¼ï¼Œä¸”åŒ…å«è›‹ç™½è´¨åºåˆ—åˆ—")
        return None, None, None, None, None


# ==========================================
# 4. æ¨¡å‹åŠ è½½
# ==========================================
@st.cache_resource
def load_model_and_scaler():
    """åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨ç¼“å­˜æé«˜æ€§èƒ½"""
    import numpy as np
    import numpy.core.multiarray
    import sklearn.preprocessing._data


    safe_globals = [
        np.core.multiarray.scalar,
        np.dtype,
        np.ndarray,
        StandardScaler,
        sklearn.preprocessing._data.StandardScaler
    ]
    for obj in safe_globals:
        try:
            torch.serialization.add_safe_globals([obj])
        except Exception as e:
            st.warning(f"æ— æ³•æ·»åŠ å®‰å…¨å…¨å±€å˜é‡: {str(e)}")


    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    st.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


    model_path = "best_multiclass_model.pth"
    if not os.path.exists(model_path):
        st.error(f"æ¨¡å‹æ–‡ä»¶ {model_path} æœªæ‰¾åˆ°ï¼è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­ã€‚")
        st.stop()


    try:
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)
        st.success("âœ… æ¨¡å‹å®‰å…¨åŠ è½½æˆåŠŸ (ä½¿ç”¨weights_only=True)")
    except Exception as e:
        try:
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        except Exception as e2:
            st.error(f"âŒ ä¸¤ç§åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {str(e2)}")
            st.stop()


    virus_map = checkpoint.get('virus_map', {
        0: "Adenovirus",
        1: "Herpesvirus",
        2: "Orthomyxovirus",
        3: "Papillomavirus",
        4: "Picornavirus",
        5: "Polyomavirus",
        6: "Rotavirus",
        7: "Coronavirus"
    })
    st.info(f"ç—…æ¯’ç±»åˆ«æ˜ å°„: {', '.join(virus_map.values())}")


    model = MutualLearningModel(input_dim=480, num_classes=8).to(device) # ä¿®æ”¹ input_dim
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    scaler = checkpoint['scaler']
    return model, scaler, virus_map, device


# ==========================================
# 5. é¢„æµ‹å’Œå¯è§†åŒ–å‡½æ•°
# ==========================================
def predict(model, scaler, sequences, device, virus_map):
    """è¿›è¡Œé¢„æµ‹"""
    extractor = ESMFeatureExtractor()
    st.info("ğŸ§¬ æ­£åœ¨æå–ESM-2ç‰¹å¾ï¼Œè¯·ç¨å€™...")
    features = extractor.extract_features(sequences)
    st.info("âš–ï¸ æ ‡å‡†åŒ–ç‰¹å¾...")
    scaled_features = scaler.transform(features)
    st.info("ğŸ§  è¿›è¡Œé¢„æµ‹...")
    model.eval()
    results = []
    with torch.no_grad():
        for i in range(len(scaled_features)):
            x = torch.FloatTensor(scaled_features[i:i+1]).to(device)
            _, _, _, fused_output = model(x)
            probs = F.softmax(fused_output, dim=1).cpu().numpy()[0]
            pred_idx = np.argmax(probs)
            results.append({
                'probabilities': probs,
                'predicted_class': virus_map[pred_idx],
                'confidence': probs[pred_idx]
            })
    return results


def create_probability_chart(probs, virus_map, title="ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ"):
    """ä½¿ç”¨çº¯matplotlibåˆ›å»ºæ¦‚ç‡åˆ†å¸ƒå›¾"""
    fig, ax = plt.subplots(figsize=(10, 5))
    viruses = [virus_map[i] for i in range(len(probs))]
    colors = ['red' if i == np.argmax(probs) else 'steelblue' for i in range(len(probs))]
    bars = ax.bar(viruses, probs, color=colors, edgecolor='black', linewidth=0.8)
    ax.set_ylim(0, 1.05)
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_ylabel('é¢„æµ‹æ¦‚ç‡', fontsize=12)
    ax.grid(axis='y', linestyle='--', alpha=0.3)
    for bar, prob in zip(bars, probs):
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width()/2., height + 0.02,
            f'{prob:.2f}',
            ha='center', va='bottom',
            fontsize=9, fontweight='bold'
        )
    plt.xticks(rotation=30, ha='right')
    plt.tight_layout()
    return fig


# ==========================================
# 6. Streamlit åº”ç”¨ä¸»å‡½æ•°
# ==========================================
def main():
    st.set_page_config(
        page_title="ç—…æ¯’è›‹ç™½åˆ†ç±»å™¨",
        page_icon="ğŸ¦ ",
        layout="wide"
    )
    st.title("ğŸ¦  ç—…æ¯’è›‹ç™½å¤šåˆ†ç±»é¢„æµ‹ç³»ç»Ÿ")
    st.markdown("""
    è¯¥ç³»ç»Ÿä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹ç—…æ¯’è›‹ç™½åºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œæ”¯æŒ8ç§ç—…æ¯’å®¶æ—çš„è¯†åˆ«ã€‚
    è¯·ä¸Šä¼ åŒ…å«è›‹ç™½è´¨åºåˆ—çš„CSVæ–‡ä»¶æˆ–ç›´æ¥è¾“å…¥å•æ¡åºåˆ—è¿›è¡Œé¢„æµ‹ã€‚
    """)


    with st.spinner("â³ åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶..."):
        try:
            model, scaler, virus_map, device = load_model_and_scaler()
        except Exception as e:
            st.error(f"åŠ è½½æ¨¡å‹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
            st.stop()


    st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")


    tab1, tab2, tab3 = st.tabs(["ğŸ”¬ å•åºåˆ—é¢„æµ‹", "ğŸ“ æ‰¹é‡é¢„æµ‹ (CSV)", "â„¹ï¸ å…³äºæ¨¡å‹"])


    with tab1:
        st.header("å•åºåˆ—é¢„æµ‹")
        sequence_input = st.text_area(
            "è¾“å…¥è›‹ç™½è´¨åºåˆ— (æ°¨åŸºé…¸åºåˆ—)",
            height=150,
            placeholder="ä¾‹å¦‚: MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHF..."
        )


        if st.button("ğŸš€ é¢„æµ‹", type="primary", use_container_width=True):
            if not sequence_input.strip():
                st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„è›‹ç™½è´¨åºåˆ—")
            else:
                is_valid, message = validate_sequence(sequence_input)
                if not is_valid:
                    st.error(f"âŒ åºåˆ—æ— æ•ˆ: {message}")
                else:
                    with st.spinner("â³ å¤„ç†ä¸­..."):
                        start_time = time.time()
                        results = predict(model, scaler, [sequence_input], device, virus_map)
                        elapsed_time = time.time() - start_time


                    res = results[0]
                    st.subheader("ğŸ¯ é¢„æµ‹ç»“æœ")
                    col1, col2 = st.columns([1, 2])
                    with col1:
                        st.metric(
                            "é¢„æµ‹ç—…æ¯’å®¶æ—",
                            res['predicted_class'],
                            delta=f"{res['confidence']:.1%} ç½®ä¿¡åº¦"
                        )
                        st.caption(f"â±ï¸ å¤„ç†æ—¶é—´: {elapsed_time:.2f} ç§’")
                    with col2:
                        fig = create_probability_chart(
                            res['probabilities'],
                            virus_map,
                            f"åºåˆ—é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ (ç½®ä¿¡åº¦: {res['confidence']:.1%})"
                        )
                        st.pyplot(fig)


                    st.subheader("ğŸ“Š è¯¦ç»†æ¦‚ç‡")
                    prob_df = pd.DataFrame({
                        'ç—…æ¯’å®¶æ—': [virus_map[i] for i in range(8)],
                        'æ¦‚ç‡': res['probabilities'] # ä¿ç•™ä¸º float
                    }).sort_values('æ¦‚ç‡', ascending=False).reset_index(drop=True)


                    # å®‰å…¨æ ¼å¼åŒ–ï¼šä»…å¯¹æ•°å€¼åˆ—åº”ç”¨æ ¼å¼
                    st.dataframe(
                        prob_df.style.format({'æ¦‚ç‡': '{:.4f}'}),
                        use_container_width=True
                    )


    with tab2:
        st.header("æ‰¹é‡é¢„æµ‹ (CSVæ ¼å¼)")
        st.markdown("""
        **ä¸Šä¼ åŒ…å«è›‹ç™½è´¨åºåˆ—çš„CSVæ–‡ä»¶**
        âœ… å¿…éœ€åˆ—: åŒ…å«æ°¨åŸºé…¸åºåˆ—çš„åˆ—ï¼ˆåˆ—åå¦‚ `Sequence`, `Protein_Sequence`, `seq` ç­‰ï¼‰
        âœ… å¯é€‰åˆ—: åºåˆ—æ ‡è¯†åˆ—ï¼ˆåˆ—åå¦‚ `Name`, `ID`, `Accession` ç­‰ï¼‰


        **CSVç¤ºä¾‹:**
        ```csv
        Name,Sequence
        Spike_1,MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHF...
        Capsid_2,MKLKKKVVVAVVAVVAGVFVAAVAGVFAAAGVFAAGVFAAGVFAAGVFAAGVFAAGVFAAGVFAAGV...
        ```
        """)


        uploaded_file = st.file_uploader(
            "ğŸ“¤ ä¸Šä¼ CSVæ–‡ä»¶ (åŒ…å«Sequenceåˆ—)",
            type=["csv"],
            help="CSVæ–‡ä»¶å¿…é¡»åŒ…å«è›‹ç™½è´¨åºåˆ—åˆ—ï¼Œåˆ—åå¯ä¸ºSequence/Seq/Protein_Sequenceç­‰"
        )


        if uploaded_file is not None:
            seq_names, sequences, raw_df, seq_col, name_col = parse_csv_sequences(uploaded_file)
            if sequences is None or len(sequences) == 0:
                st.stop()


            with st.expander("ğŸ” CSVæ•°æ®é¢„è§ˆ (å‰10è¡Œ)"):
                preview_df = raw_df.head(10).copy()
                st.dataframe(preview_df, use_container_width=True)


            name_info = "æœªæ£€æµ‹åˆ°åç§°åˆ—ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨ç¼–å·" if name_col is None else f"åç§°åˆ—: {name_col}"
            st.caption(f"æ£€æµ‹åˆ°åºåˆ—åˆ—: '{seq_col}' | {name_info}")
            st.info(f"ğŸ“Š å…±æ£€æµ‹åˆ° {len(sequences)} ä¸ªæœ‰æ•ˆåºåˆ—")


            if st.button("ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹", type="primary", use_container_width=True):
                valid_indices, errors = validate_csv_sequences(sequences, seq_names)
                if errors:
                    st.error(f"âŒ å‘ç° {len(errors)} ä¸ªæ— æ•ˆåºåˆ—:")
                    for name, msg in errors[:10]:
                        st.write(f"- **{name}**: {msg}")
                    if len(errors) > 10:
                        st.write(f"... è¿˜æœ‰ {len(errors)-10} ä¸ªé”™è¯¯æœªæ˜¾ç¤º")
                    st.stop()


                if len(valid_indices) > 50:
                    st.warning(f"âš ï¸ æ‚¨ä¸Šä¼ äº† {len(valid_indices)} ä¸ªåºåˆ—ï¼Œå¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")


                with st.spinner(f"â³ æ­£åœ¨é¢„æµ‹ {len(valid_indices)} ä¸ªåºåˆ—..."):
                    start_time = time.time()
                    valid_seqs = [sequences[i] for i in valid_indices]
                    valid_names = [seq_names[i] for i in valid_indices]
                    results = predict(model, scaler, valid_seqs, device, virus_map)
                    total_time = time.time() - start_time


                # ====== ä¿®å¤æ ¸å¿ƒï¼šä¿ç•™æ•°å€¼ç±»å‹ï¼Œä¸åœ¨æ„å»ºæ—¶è½¬å­—ç¬¦ä¸² ======
                results_data = []
                for i, (name, res) in enumerate(zip(valid_names, results)):
                    row = {
                        'åºåˆ—åç§°': name,
                        'é¢„æµ‹ç—…æ¯’': res['predicted_class'],
                        'ç½®ä¿¡åº¦': res['confidence'] # ä¿ç•™ä¸º float
                    }
                    # æ·»åŠ æ‰€æœ‰ç—…æ¯’å®¶æ—æ¦‚ç‡ï¼ˆä¿ç•™ä¸º floatï¼‰
                    for j in range(8):
                        row[virus_map[j]] = res['probabilities'][j] # å…³é”®ä¿®å¤ï¼šä¸è½¬å­—ç¬¦ä¸²
                    results_data.append(row)


                results_df = pd.DataFrame(results_data)


                st.subheader("ğŸ“ˆ é¢„æµ‹ç»“æœæ±‡æ€»")
                st.caption(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ | å¹³å‡: {total_time/len(valid_indices):.2f} ç§’/åºåˆ—")


                # ====== å®‰å…¨æ ¼å¼åŒ–ï¼šæ˜¾å¼æ„å»ºæ ¼å¼åŒ–å­—å…¸ ======
                format_dict = {'ç½®ä¿¡åº¦': '{:.2%}'} # ç½®ä¿¡åº¦æ˜¾ç¤ºä¸ºç™¾åˆ†æ¯”
                # ä¸ºæ‰€æœ‰ç—…æ¯’å®¶æ—åˆ—æ·»åŠ æ ¼å¼ï¼ˆæ’é™¤éæ•°å€¼åˆ—ï¼‰
                for col in results_df.columns:
                    if col not in ['åºåˆ—åç§°', 'é¢„æµ‹ç—…æ¯’', 'ç½®ä¿¡åº¦']:
                        format_dict[col] = '{:.4f}'


                # åº”ç”¨æ ¼å¼åŒ–ï¼ˆæ·»åŠ  na_rep å¤„ç†æ½œåœ¨ç¼ºå¤±å€¼ï¼‰
                styled_df = results_df.style.format(format_dict, na_rep='N/A')
                st.dataframe(styled_df, use_container_width=True)


                st.subheader("ğŸ“Š å¯è§†åŒ–é€‰é¡¹")
                col1, col2 = st.columns(2)
                with col1:
                    show_chart = st.checkbox("æ˜¾ç¤ºæ‰€æœ‰åºåˆ—é¢„æµ‹æ¦‚è§ˆ", value=True)
                with col2:
                    if len(valid_names) > 1:
                        show_details = st.checkbox("æŸ¥çœ‹å•ä¸ªåºåˆ—è¯¦ç»†åˆ†å¸ƒ")


                if show_chart and len(valid_indices) <= 20:
                    fig, ax = plt.subplots(figsize=(12, 6))
                    x = np.arange(len(virus_map))
                    width = 0.8 / len(valid_names)
                    for i, (name, res) in enumerate(zip(valid_names, results)):
                        ax.bar(x + i*width, res['probabilities'], width, label=name)
                    ax.set_xlabel('ç—…æ¯’å®¶æ—')
                    ax.set_ylabel('é¢„æµ‹æ¦‚ç‡')
                    ax.set_title('æ‰€æœ‰åºåˆ—é¢„æµ‹æ¦‚ç‡å¯¹æ¯”')
                    ax.set_xticks(x + width * (len(valid_names)-1)/2)
                    ax.set_xticklabels([virus_map[i] for i in range(8)], rotation=30, ha='right')
                    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
                    ax.set_ylim(0, 1.05)
                    plt.tight_layout()
                    st.pyplot(fig)


                if show_details and len(valid_names) > 1:
                    selected_seq = st.selectbox(
                        "é€‰æ‹©è¦æŸ¥çœ‹è¯¦ç»†åˆ†å¸ƒçš„åºåˆ—",
                        options=valid_names,
                        key="seq_selector_csv"
                    )
                    idx = valid_names.index(selected_seq)
                    fig = create_probability_chart(
                        results[idx]['probabilities'],
                        virus_map,
                        f"{selected_seq} çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"
                    )
                    st.pyplot(fig)


                # ä¸‹è½½ä¿ç•™åŸå§‹æ•°å€¼ï¼ˆå°æ•°å½¢å¼ï¼Œä¾¿äºåç»­åˆ†æï¼‰
                csv = results_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ (CSV)",
                    data=csv,
                    file_name="virus_predictions.csv",
                    mime="text/csv",
                    use_container_width=True
                )


    with tab3:
        st.header("â„¹ï¸ å…³äºæ¨¡å‹")
        st.markdown("""
        ### ğŸ§  æ¨¡å‹æ¶æ„
        - **ä¸‰åˆ†æ”¯èåˆæ¶æ„**: CNN + Transformer + Mamba
        - **è‡ªé€‚åº”é—¨æ§èåˆ**: åŠ¨æ€åŠ æƒæ•´åˆä¸‰ä¸ªåˆ†æ”¯çš„é¢„æµ‹
        - **è¾“å…¥ç‰¹å¾**: ESM-2 (35M) æå–çš„480ç»´è›‹ç™½è´¨è¡¨ç¤º


        ### ğŸ¦  æ”¯æŒçš„ç—…æ¯’å®¶æ— (8ç±»)
        | ç¼–å· | ç—…æ¯’å®¶æ— | å¸¸è§ä»£è¡¨ |
        |------|----------|----------|
        | 0 | Adenovirus | è…ºç—…æ¯’ |
        | 1 | Herpesvirus | ç–±ç–¹ç—…æ¯’ |
        | 2 | Orthomyxovirus | æµæ„Ÿç—…æ¯’ |
        | 3 | Papillomavirus | äººä¹³å¤´ç˜¤ç—…æ¯’ |
        | 4 | Picornavirus | è‚ é“ç—…æ¯’ |
        | 5 | Polyomavirus | å¤šç˜¤ç—…æ¯’ |
        | 6 | Rotavirus | è½®çŠ¶ç—…æ¯’ |
        | 7 | Coronavirus | å† çŠ¶ç—…æ¯’ |


        ### ğŸ“Š CSVä¸Šä¼ è¯´æ˜
        - **å¿…éœ€åˆ—**: åŒ…å«æ°¨åŸºé…¸åºåˆ—çš„åˆ—ï¼ˆè‡ªåŠ¨è¯†åˆ«å¸¸è§åˆ—åï¼‰
        - **æ™ºèƒ½è¯†åˆ«**: æ”¯æŒå¤šç§åˆ—åå˜ä½“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        - **é”™è¯¯å¤„ç†**: è‡ªåŠ¨è·³è¿‡ç©ºåºåˆ—ï¼Œè¯¦ç»†æŠ¥å‘Šæ— æ•ˆåºåˆ—
        - **åç§°å¤„ç†**: ä¼˜å…ˆä½¿ç”¨IDåˆ—ï¼Œæ— IDæ—¶è‡ªåŠ¨ç”Ÿæˆåºåˆ—åç§°


        ### ğŸ”’ å®‰å…¨è¯´æ˜
        - æ¨¡å‹åŠ è½½ä½¿ç”¨ PyTorch `weights_only=True` å®‰å…¨æ¨¡å¼
        - é€šè¿‡ `torch.serialization.add_safe_globals()` å®‰å…¨åŠ è½½ StandardScaler
        - æ‰€æœ‰é¢„æµ‹åœ¨æœ¬åœ°å®Œæˆï¼Œæ•°æ®ä¸ä¼šä¸Šä¼ åˆ°å¤–éƒ¨æœåŠ¡å™¨


        ### ğŸ“¦ ä¾èµ–è¦æ±‚
        ```bash
        pip install streamlit torch esm mamba-ssm pandas numpy scikit-learn matplotlib
        ```
        """)


if __name__ == "__main__":
    main()
