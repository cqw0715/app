import streamlit as st
import torch
import numpy as np
import pandas as pd
import os
import pickle
from io import StringIO
from torch.utils.data import DataLoader, TensorDataset
import esm
import time
from typing import List, Tuple, Optional, Union
from tqdm import tqdm

# ==========================================
# æ¨¡å‹æ¶æ„å®šä¹‰
# ==========================================
import torch.nn as nn
import torch.nn.functional as F

try:
    from mamba_ssm import Mamba
except ImportError:
    st.warning("Mambaæ¨¡å—æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ›¿ä»£å®ç°")

    # ç®€å•çš„æ›¿ä»£å®ç°ï¼Œä»…ç”¨äºæ¼”ç¤º
    class Mamba(nn.Module):
        def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
            super().__init__()
            self.d_model = d_model
            self.norm = nn.LayerNorm(d_model)

        def forward(self, x):
            return self.norm(x)


class CNNBranch(nn.Module):
    def __init__(self, input_dim=480, num_classes=2):  # input_dim
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.Unflatten(1, (1, 256)),
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Conv1d(256, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.AdaptiveMaxPool1d(1)
        )
        self.classifier = nn.Linear(128, num_classes)

    def forward(self, x):
        feat = self.net(x).flatten(1)
        return self.classifier(feat)


class TransformerBranch(nn.Module):
    def __init__(self, input_dim=480, d_model=256, nhead=8, num_classes=2):  # input_dim
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, dropout=0.2)
        self.transformer = nn.TransformerEncoder(layer, num_layers=4)
        self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, x):
        x = self.embedding(x).unsqueeze(1)
        x = self.transformer(x).squeeze(1)
        return self.classifier(x)


class MambaBranch(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.preprocess = nn.Linear(input_dim, 256)
        self.mamba_blocks = nn.ModuleList([
            Mamba(d_model=256, d_state=16, d_conv=4, expand=2) for _ in range(5)
        ])
        self.norm = nn.LayerNorm(256)
        self.classifier = nn.Linear(256, num_classes)

    def forward(self, x):
        x = self.preprocess(x).unsqueeze(1)
        for block in self.mamba_blocks:
            x = x + block(x)
        x = self.norm(x).squeeze(1)
        return self.classifier(x)


class MutualLearningModel(nn.Module):
    def __init__(self, input_dim=480, num_classes=2, embed_dim=128):  # input_dim
        super().__init__()
        self.cnn = CNNBranch(input_dim, num_classes)
        self.trans = TransformerBranch(input_dim, num_classes=num_classes)
        self.mamba = MambaBranch(input_dim, num_classes)
        self.logits_norm = nn.LayerNorm(num_classes)
        self.feature_proj = nn.Sequential(
            nn.Linear(num_classes, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        self.attn1 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2, batch_first=True)
        self.attn_norm1 = nn.LayerNorm(embed_dim)
        self.ffn1 = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(embed_dim * 4, embed_dim)
        )
        self.ffn_norm1 = nn.LayerNorm(embed_dim)
        self.attn2 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2, batch_first=True)
        self.attn_norm2 = nn.LayerNorm(embed_dim)
        self.ffn2 = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(embed_dim * 4, embed_dim)
        )
        self.ffn_norm2 = nn.LayerNorm(embed_dim)
        total_gate_dim = embed_dim * 3 + num_classes * 3
        self.gate = nn.Sequential(
            nn.Linear(total_gate_dim, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(128, 3)
        )
        self.log_temp = nn.Parameter(torch.tensor(np.log(0.8 + 1e-6)))
        self.refine = nn.Sequential(
            nn.Linear(num_classes, num_classes),
            nn.LayerNorm(num_classes),
            nn.GELU(),
            nn.Dropout(0.1)
        )

    def forward(self, x):
        o1, o2, o3 = self.cnn(x), self.trans(x), self.mamba(x)
        branches = torch.stack([o1, o2, o3], dim=1)
        branches_norm = self.logits_norm(branches)
        x_proj = self.feature_proj(branches_norm)
        attn_out, _ = self.attn1(x_proj, x_proj, x_proj)
        x = self.attn_norm1(x_proj + attn_out)
        x = self.ffn_norm1(x + self.ffn1(x))
        attn_out, _ = self.attn2(x, x, x)
        x = self.attn_norm2(x + attn_out)
        x = self.ffn_norm2(x + self.ffn2(x))
        raw_logits = branches.flatten(1)
        fused_proj = x.flatten(1)
        combined_feat = torch.cat([fused_proj, raw_logits], dim=1)
        gate_scores = self.gate(combined_feat)
        temp = F.softplus(self.log_temp) + 1e-4
        weights = F.softmax(gate_scores / temp, dim=1).unsqueeze(-1)
        o_fused = (branches * weights).sum(dim=1)
        o_fused = o_fused + self.refine(o_fused)
        return o1, o2, o3, o_fused


# ==========================================
# ç‰¹å¾æå–ç±»
# ==========================================
class ESMFeatureExtractor:
    def __init__(self):
        self.gpu_model = None
        self.gpu_batch_converter = None
        self.cpu_model = None
        self.cpu_batch_converter = None
        self.device = None
        self._initialize_models()

    def _initialize_models(self):
        try:
            if torch.cuda.is_available():
                print("ğŸš€ å°è¯•åŠ è½½GPUæ¨¡å‹ï¼ˆESM-2 35Mï¼‰...")
                self.gpu_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()  # æ›¿æ¢ä¸º 35M æ¨¡å‹
                self.gpu_device = torch.device('cuda')
                self.gpu_model = self.gpu_model.to(self.gpu_device)
                self.gpu_batch_converter = alphabet.get_batch_converter()
                self.device = self.gpu_device
                print("âœ… GPUæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ GPUæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        try:
            print("ğŸ–¥ï¸ åŠ è½½CPUæ¨¡å‹ä½œä¸ºå¤‡ç”¨...")
            self.cpu_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()  # æ›¿æ¢ä¸º 35M æ¨¡å‹
            self.cpu_device = torch.device('cpu')
            self.cpu_model = self.cpu_model.to(self.cpu_device)
            self.cpu_batch_converter = alphabet.get_batch_converter()
            if self.device is None:
                self.device = self.cpu_device
            print("âœ… CPUæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ CPUæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _extract_batch_features(self, batch_data, use_gpu=True):
        try:
            model = self.gpu_model if use_gpu and self.gpu_model else self.cpu_model
            batch_converter = self.gpu_batch_converter if use_gpu and self.gpu_model else self.cpu_batch_converter
            device = self.gpu_device if use_gpu and self.gpu_model else self.cpu_device

            _, _, batch_tokens = batch_converter(batch_data)
            batch_tokens = batch_tokens.to(device)
            with torch.no_grad():
                results = model(batch_tokens, repr_layers=[6], return_contacts=False)  # ä¿®æ”¹ä¸ºç¬¬6å±‚
                token_representations = results["representations"][6]  # ä¿®æ”¹ä¸ºç¬¬6å±‚
            seq_lengths = (batch_tokens != model.alphabet.padding_idx).sum(1)
            batch_features = [token_representations[i, :seq_lengths[i]].mean(0).cpu().numpy() for i in
                              range(token_representations.size(0))]

            del batch_tokens, results
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return batch_features
        except RuntimeError as e:
            if "CUDA out of memory" in str(e) and use_gpu:
                return self._extract_batch_features(batch_data, use_gpu=False)
            raise

    def extract_features(self, sequences, cache_path=None, batch_size=1):
        if cache_path and os.path.exists(cache_path):
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½ç‰¹å¾: {cache_path}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)

        features = []
        for i in range(0, len(sequences), batch_size):
            batch = sequences[i:i + batch_size]
            batch_data = [(str(idx), seq) for idx, seq in enumerate(batch)]
            features.extend(self._extract_batch_features(batch_data))

            if (i // batch_size) % 10 == 0:
                print(f"ğŸ“Š è¿›åº¦: {min(i + batch_size, len(sequences))}/{len(sequences)}")

        features_array = np.array(features)
        if cache_path:
            with open(cache_path, 'wb') as f:
                pickle.dump(features_array, f)
        return features_array


# ==========================================
# ç¼“å­˜å‡½æ•° - æ­£ç¡®ä½¿ç”¨st.cache_resource
# ==========================================
@st.cache_resource
def get_feature_extractor():
    """è·å–ç‰¹å¾æå–å™¨çš„ç¼“å­˜å®ä¾‹"""
    return ESMFeatureExtractor()


@st.cache_resource
def load_model_and_scaler():
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨"""
    with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹..."):
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = "best_mutual_learning_model.pth"
        if not os.path.exists(model_path):
            st.error(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
            st.info("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶ä¸åº”ç”¨åœ¨åŒä¸€ç›®å½•ä¸‹")
            return None, None, None

        # åŠ è½½æ¨¡å‹ - ä¿®å¤å®‰å…¨è­¦å‘Š
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        try:
            # å°è¯•ä½¿ç”¨ weights_only=True (PyTorch 2.1+)
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        except TypeError:
            # æ—§ç‰ˆæœ¬PyTorchä¸æ”¯æŒweights_onlyå‚æ•°
            checkpoint = torch.load(model_path, map_location=device)

        # åˆå§‹åŒ–æ¨¡å‹
        model = MutualLearningModel(input_dim=480, num_classes=2).to(device)  # ä¿®æ”¹ input_dim
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        # è·å–æ ‡å‡†åŒ–å™¨
        scaler = checkpoint['scaler']
        return model, scaler, device


# ==========================================
# åº”ç”¨ä¸»å‡½æ•°
# ==========================================
def main():
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="çŒªè‚ é“ç—…æ¯’è¯†åˆ«ç³»ç»Ÿ",
        page_icon="ğŸ·",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # åº”ç”¨æ ‡é¢˜å’Œè¯´æ˜
    st.title("ğŸ· çŒªè‚ é“ç—…æ¯’è¯†åˆ«ç³»ç»Ÿ")
    st.markdown("""
    <div style="background-color: #f0f2f6; padding: 15px; border-radius: 10px; margin-bottom: 20px;">
    <h3>ğŸ”¬ ç³»ç»Ÿè¯´æ˜</h3>
    <p>æœ¬ç³»ç»Ÿä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹è›‹ç™½è´¨åºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸ºçŒªè‚ é“ç—…æ¯’ã€‚</p>
    <ul>
    <li><b>ç±»åˆ«0</b>: çŒªè‚ é“ç—…æ¯’</li>
    <li><b>ç±»åˆ«1</b>: éçŒªè‚ é“ç—…æ¯’</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
        st.markdown("### æ¨¡å‹ä¿¡æ¯")
        st.info("ç‰¹å¾æå–æ–¹æ³•\n(ESM-2)")
        st.info("å‚ä¸èåˆæ¨¡å‹\n(CNN + Transformer + Mamba)")
        st.markdown("### ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        1. **å•åºåˆ—é¢„æµ‹**: åœ¨è¾“å…¥æ¡†ä¸­ç²˜è´´è›‹ç™½è´¨åºåˆ—
        2. **æ‰¹é‡é¢„æµ‹**: ä¸Šä¼ åŒ…å«åºåˆ—çš„CSVæ–‡ä»¶
        3. æŸ¥çœ‹é¢„æµ‹ç»“æœåŠç½®ä¿¡åº¦
        """)
        st.markdown("### æ³¨æ„äº‹é¡¹")
        st.warning("""
        - ä»…æ”¯æŒæ ‡å‡†æ°¨åŸºé…¸å­—ç¬¦
        - åºåˆ—é•¿åº¦å»ºè®®åœ¨50-2000ä¸ªæ°¨åŸºé…¸ä¹‹é—´
        - GPUåŠ é€Ÿå¯æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦
        """)
        st.warning("""
        - ç”±äºæ¨¡å‹è¿è¡Œåœ¨æœåŠ¡å™¨CPUä¸Šï¼Œä¼šå¯¼è‡´è¿è¡Œé”™è¯¯ï¼Œæœ¬ç³»ç»Ÿä»…å±•ç¤ºç›¸å…³ç³»ç»ŸåŠŸèƒ½ï¼Œå¹¶ä¸”è¯¥æœåŠ¡å™¨åªæœ‰1Gçš„å†…å­˜ï¼Œè€ŒESM-2 650Méœ€è¦3Gä»¥ä¸Šçš„å†…å­˜ï¼Œç”±äºå†…å­˜ä¸è¶³ï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨çš„æ˜¯ESM-3 35Mç‰ˆæœ¬ï¼Œä½†æ»¡è¡€ç‰ˆé¢„æµ‹ç³»ç»Ÿå¯åœ¨æœ¬åœ°è¿›è¡Œéƒ¨ç½²ï¼Œè¯·å‚è€ƒï¼ˆgithubé“¾æ¥ï¼‰è¿›è¡Œæœ¬åœ°éƒ¨ç½²
        - ç»è´¹å……è¶³æ—¶ä¼šè€ƒè™‘ç§Ÿå€Ÿæ›´å¥½çš„æœåŠ¡å™¨ä»¥æ»¡è¶³åœ¨çº¿è¯†åˆ«çš„éœ€æ±‚
        """)

    # åŠ è½½æ¨¡å‹å’Œç‰¹å¾æå–å™¨
    model, scaler, device = load_model_and_scaler()
    feature_extractor = get_feature_extractor()

    if model is None or feature_extractor is None:
        st.stop()

    # é¢„æµ‹å‡½æ•°
    def predict_sequences(sequences: List[str]) -> List[dict]:
        """å¯¹åºåˆ—åˆ—è¡¨è¿›è¡Œé¢„æµ‹"""
        if not sequences:
            return []

        # æå–ç‰¹å¾
        with st.spinner(f"ğŸ§¬ æ­£åœ¨æå– {len(sequences)} æ¡åºåˆ—çš„ç‰¹å¾..."):
            features = feature_extractor.extract_features(sequences)

        # æ ‡å‡†åŒ–
        features_scaled = scaler.transform(features)
        features_tensor = torch.FloatTensor(features_scaled).to(device)

        # é¢„æµ‹
        results = []
        with torch.no_grad():
            _, _, _, o_fused = model(features_tensor)
            probs = F.softmax(o_fused, dim=1)
            preds = torch.argmax(probs, dim=1).cpu().numpy()
            confidences = probs[:, 1].cpu().numpy()  # éçŒªè‚ é“ç—…æ¯’çš„æ¦‚ç‡

        # ç”Ÿæˆç»“æœ
        for i, (seq, pred, conf) in enumerate(zip(sequences, preds, confidences)):
            result = {
                'sequence_id': f"seq_{i + 1}",
                'sequence': seq[:50] + "..." if len(seq) > 50 else seq,
                'full_sequence': seq,
                'prediction': int(pred),
                'confidence': float(conf),
                'class_name': "éçŒªè‚ é“ç—…æ¯’" if pred == 1 else "çŒªè‚ é“ç—…æ¯’"
            }
            results.append(result)
        return results

    # ä¸»ç•Œé¢ - ä¸¤ç§è¾“å…¥æ–¹å¼
    input_option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼", ["å•åºåˆ—é¢„æµ‹", "æ‰¹é‡CSVé¢„æµ‹"], horizontal=True)

    if input_option == "å•åºåˆ—é¢„æµ‹":
        st.subheader("ğŸ”¤ è¾“å…¥è›‹ç™½è´¨åºåˆ—")
        sequence_input = st.text_area(
            "ç²˜è´´è›‹ç™½è´¨åºåˆ— (ä»…æ”¯æŒæ ‡å‡†æ°¨åŸºé…¸å­—ç¬¦)",
            height=150,
            placeholder="ä¾‹å¦‚: MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDMKHLKKADLIICAPNSYKKDDKPNQIKLLAVPTVMTKDDKQLLQEINELQDVVQDLRSLVEKNQIPAVDRAVTLTQRGELQAAGDKTLQEAVDRLQDKLQSLAEEGVKALQEELRKQLEAVDRAVTKLEQKLQDQVEALQARVDSLQAELRALQAQLAELQAELQALRSQLDELQAQLAELQAQLQALQSELQAQLSQLDELQAQLAELQAQLQALQSELQAQLSQLDELQAQLAELQAQLQALQSELQAQLSQLDELQAQLAELQAQLQ"
        )

        if st.button("ğŸ” å¼€å§‹é¢„æµ‹", type="primary"):
            if not sequence_input.strip():
                st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„è›‹ç™½è´¨åºåˆ—")
            else:
                # é¢„å¤„ç†åºåˆ— - åªä¿ç•™æ ‡å‡†æ°¨åŸºé…¸
                sequence = ''.join(filter(str.isalpha, sequence_input.strip().upper()))
                sequence = ''.join([aa for aa in sequence if aa in 'ACDEFGHIKLMNPQRSTVWYX'])
                if len(sequence) < 10:
                    st.error("âŒ åºåˆ—é•¿åº¦è¿‡çŸ­ï¼Œè¯·è¾“å…¥è‡³å°‘10ä¸ªæ°¨åŸºé…¸çš„åºåˆ—")
                elif len(sequence) > 10000:
                    st.error("âŒ åºåˆ—é•¿åº¦è¿‡é•¿ï¼Œæœ€å¤§æ”¯æŒ10000ä¸ªæ°¨åŸºé…¸")
                else:
                    # è¿›è¡Œé¢„æµ‹
                    results = predict_sequences([sequence])

                    # æ˜¾ç¤ºç»“æœ
                    st.subheader("ğŸ“Š é¢„æµ‹ç»“æœ")
                    result = results[0]

                    # ä½¿ç”¨å¡ç‰‡å¼å¸ƒå±€å±•ç¤ºç»“æœ
                    if result['prediction'] == 0:
                        color = "#ff4b4b"  # çº¢è‰²è¡¨ç¤ºçŒªè‚ é“ç—…æ¯’
                        emoji = "ğŸ·"
                    else:
                        color = "#1f77b4"  # è“è‰²è¡¨ç¤ºéçŒªè‚ é“ç—…æ¯’
                        emoji = "ğŸ¦ "

                    st.markdown(f"""
                    <div style="background-color: {color}15; border-left: 4px solid {color}; padding: 15px; border-radius: 0 8px 8px 0; margin: 15px 0;">
                    <h3 style="color: {color};">{emoji} é¢„æµ‹ç»“æœ: {result['class_name']}</h3>
                    <p><b>ç½®ä¿¡åº¦:</b> {result['confidence']:.2%}</p>
                    <p><b>åºåˆ—é¢„è§ˆ:</b> {result['sequence']}</p>
                    </div>
                    """, unsafe_allow_html=True)

                    # æ˜¾ç¤ºè¯¦ç»†ç½®ä¿¡åº¦
                    st.subheader("ğŸ“ˆ ç½®ä¿¡åº¦åˆ†æ")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("çŒªè‚ é“ç—…æ¯’æ¦‚ç‡", f"{1 - result['confidence']:.2%}")
                    with col2:
                        st.metric("éçŒªè‚ é“ç—…æ¯’æ¦‚ç‡", f"{result['confidence']:.2%}")

                    # å¯è§†åŒ–ç½®ä¿¡åº¦
                    import matplotlib.pyplot as plt
                    fig, ax = plt.subplots(figsize=(8, 2))
                    classes = ['PEV', 'non-PEV']
                    probabilities = [1 - result['confidence'], result['confidence']]
                    colors = ['#ff4b4b', '#1f77b4']
                    bars = ax.barh(classes, probabilities, color=colors)
                    ax.set_xlim(0, 1)
                    ax.set_title('Forecast probability distribution')
                    ax.bar_label(bars, fmt='%.2f', padding=3)
                    st.pyplot(fig)

                    # æ˜¾ç¤ºå®Œæ•´åºåˆ—
                    with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´åºåˆ—"):
                        st.code(result['full_sequence'])

    else:  # æ‰¹é‡CSVé¢„æµ‹
        st.subheader("ğŸ“ ä¸Šä¼ CSVæ–‡ä»¶")
        st.markdown("""
        è¯·ä¸Šä¼ åŒ…å«è›‹ç™½è´¨åºåˆ—çš„CSVæ–‡ä»¶ï¼Œæ–‡ä»¶éœ€åŒ…å«`Sequence`åˆ—ã€‚
        **ç¤ºä¾‹æ ¼å¼:**
        ```
        ID,Sequence
        seq1,MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHF
        seq2,MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGV
        ```
        """)
        uploaded_file = st.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type=["csv"])

        if uploaded_file is not None:
            try:
                # è¯»å–CSV
                df = pd.read_csv(uploaded_file)
                if 'Sequence' not in df.columns:
                    st.error("âŒ CSVæ–‡ä»¶ä¸­ç¼ºå°‘'Sequence'åˆ—")
                else:
                    st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡åºåˆ—")

                    # é¢„è§ˆæ•°æ®
                    with st.expander("ğŸ” æ•°æ®é¢„è§ˆ"):
                        st.dataframe(df.head())

                    # é¢„å¤„ç†åºåˆ—
                    sequences = []
                    valid_indices = []
                    for idx, row in df.iterrows():
                        seq = str(row['Sequence']).strip().upper()
                        # ä»…ä¿ç•™æ ‡å‡†æ°¨åŸºé…¸
                        seq_clean = ''.join([aa for aa in seq if aa in 'ACDEFGHIKLMNPQRSTVWYX'])
                        if len(seq_clean) >= 10 and len(seq_clean) <= 10000:
                            sequences.append(seq_clean)
                            valid_indices.append(idx)

                    st.info(f"â„¹ï¸ æœ‰æ•ˆåºåˆ—: {len(sequences)}/{len(df)} (è¿‡æ»¤äº†è¿‡çŸ­ã€è¿‡é•¿æˆ–æ— æ•ˆå­—ç¬¦çš„åºåˆ—)")

                    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹", type="primary"):
                        if not sequences:
                            st.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åºåˆ—å¯ä»¥é¢„æµ‹")
                        else:
                            # è¿›è¡Œé¢„æµ‹
                            with st.spinner(f"ğŸ§  æ­£åœ¨é¢„æµ‹ {len(sequences)} æ¡åºåˆ—..."):
                                start_time = time.time()
                                results = predict_sequences(sequences)
                                elapsed_time = time.time() - start_time

                            # åˆ›å»ºç»“æœDataFrame
                            results_df = pd.DataFrame(results)
                            results_df = results_df[['sequence_id', 'sequence', 'prediction', 'confidence', 'class_name']]

                            # ä¸åŸå§‹æ•°æ®åˆå¹¶
                            result_indices = pd.Series(valid_indices, name='original_index')
                            results_with_index = pd.concat([result_indices, results_df], axis=1)

                            # åˆ›å»ºå®Œæ•´çš„è¾“å‡º
                            output_df = df.copy()
                            output_df['Prediction'] = "æ— æ•ˆåºåˆ—"
                            output_df['Class'] = "æ— æ•ˆåºåˆ—"
                            output_df['Confidence'] = 0.0
                            for _, row in results_with_index.iterrows():
                                idx = int(row['original_index'])
                                output_df.at[idx, 'Prediction'] = row['prediction']
                                output_df.at[idx, 'Class'] = row['class_name']
                                output_df.at[idx, 'Confidence'] = row['confidence']

                            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                            st.subheader("ğŸ“Š é¢„æµ‹ç»Ÿè®¡")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                total_valid = len(sequences)
                                st.metric("æœ‰æ•ˆåºåˆ—æ•°", total_valid)
                            with col2:
                                pig_virus_count = sum(1 for r in results if r['prediction'] == 0)
                                st.metric("çŒªè‚ é“ç—…æ¯’", pig_virus_count)
                            with col3:
                                non_pig_count = total_valid - pig_virus_count
                                st.metric("éçŒªè‚ é“ç—…æ¯’", non_pig_count)

                            st.success(f"âœ… é¢„æµ‹å®Œæˆ! è€—æ—¶: {elapsed_time:.2f} ç§’")

                            # æ˜¾ç¤ºç»“æœé¢„è§ˆ
                            st.subheader("ğŸ” ç»“æœé¢„è§ˆ")
                            st.dataframe(output_df.head(10))

                            # ä¸‹è½½ç»“æœ
                            csv = output_df.to_csv(index=False).encode('utf-8')
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½å®Œæ•´ç»“æœ (CSV)",
                                data=csv,
                                file_name="prediction_results.csv",
                                mime="text/csv",
                                type="primary"
                            )

                            # å¯è§†åŒ–
                            st.subheader("ğŸ“ˆ ç»“æœåˆ†å¸ƒ")
                            import matplotlib.pyplot as plt
                            fig, ax = plt.subplots(figsize=(10, 6))
                            class_counts = output_df[output_df['Prediction'] != "æ— æ•ˆåºåˆ—"]['Class'].value_counts()
                            colors = ['#ff4b4b', '#1f77b4']
                            bars = class_counts.plot(kind='bar', color=colors, ax=ax)
                            ax.set_title('é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ', fontsize=16)
                            ax.set_xlabel('ç±»åˆ«', fontsize=12)
                            ax.set_ylabel('æ•°é‡', fontsize=12)
                            ax.tick_params(axis='x', rotation=0)
                            # æ·»åŠ æ•°æ®æ ‡ç­¾
                            for i, v in enumerate(class_counts.values):
                                ax.text(i, v + 0.5, str(v), ha='center', fontweight='bold')
                            st.pyplot(fig)

            except Exception as e:
                st.error(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                st.exception(e)


if __name__ == "__main__":
    main()
